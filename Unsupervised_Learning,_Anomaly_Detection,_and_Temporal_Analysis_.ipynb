{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:What is Dimensionality Reduction? Why is it important in machine**\n",
        "**learning?**\n",
        "\n",
        "Answer:-1. Introduction\n",
        "\n",
        "Dimensionality Reduction is a technique used in machine learning to reduce the number of input features (dimensions) in a dataset while preserving as much important information as possible. In simple terms, it converts high-dimensional data into a lower-dimensional form without losing critical patterns or relationships.\n",
        "\n",
        "It is used when datasets contain many features, some of which may be irrelevant, redundant, or noisy.\n",
        "\n",
        "2. What is Dimensionality?\n",
        "\n",
        "Dimensionality refers to the number of input variables or features in a dataset.\n",
        "\n",
        "Example:\n",
        "\n",
        "A dataset with height, weight, and age has 3 dimensions.\n",
        "\n",
        "Images of size 100 × 100 have 10,000 dimensions (pixels).\n",
        "\n",
        "When the number of features increases, the dataset becomes “high-dimensional”.\n",
        "\n",
        "3. What is Dimensionality Reduction?\n",
        "\n",
        "Dimensionality Reduction is the process of:\n",
        "\n",
        "Selecting only the most important features (Feature Selection), or\n",
        "\n",
        "Transforming existing features into fewer new features (Feature Extraction)\n",
        "\n",
        "so that the model becomes simpler, faster, and more accurate.\n",
        "\n",
        "Two Main Types\n",
        "\n",
        "Feature Selection – Keep only useful features\n",
        "\n",
        "Methods: Filter methods, Wrapper methods, Embedded methods\n",
        "\n",
        "Examples: Chi-square test, RFE, Lasso regression\n",
        "\n",
        "Feature Extraction – Create new features from existing ones\n",
        "\n",
        "Methods:\n",
        "\n",
        "PCA (Principal Component Analysis)\n",
        "\n",
        "LDA (Linear Discriminant Analysis)\n",
        "\n",
        "t-SNE\n",
        "\n",
        "Autoencoders\n",
        "\n",
        "4. Why is Dimensionality Reduction Important?\n",
        "(a) Removes Irrelevant and Redundant Features\n",
        "\n",
        "High-dimensional data contains unnecessary features that confuse the model. Dimensionality reduction helps remove noise and improves model clarity.\n",
        "\n",
        "(b) Reduces Overfitting\n",
        "\n",
        "When there are too many features and not enough samples, models overfit.\n",
        "Reducing dimensions helps the model generalize better to new data.\n",
        "\n",
        "(c) Improves Model Accuracy\n",
        "\n",
        "By eliminating unimportant features, the model focuses on the most informative variables, resulting in better accuracy and performance.\n",
        "\n",
        "(d) Reduces Computational Cost\n",
        "\n",
        "High-dimensional data requires more:\n",
        "\n",
        "Memory\n",
        "\n",
        "Processing power\n",
        "\n",
        "Training time\n",
        "\n",
        "Dimensionality reduction makes training faster and more efficient.\n",
        "\n",
        "(e) Helps in Data Visualization\n",
        "\n",
        "Human beings can only visualize data in:\n",
        "\n",
        "2D\n",
        "\n",
        "3D\n",
        "\n",
        "Techniques like PCA and t-SNE reduce data to 2 or 3 dimensions, making it easier to visualize clusters, patterns, and relationships.\n",
        "\n",
        "\n",
        "(f) Improves Storage and Efficiency\n",
        "\n",
        "With fewer features:\n",
        "\n",
        "Storage space reduces\n",
        "\n",
        "Data pipelines become lightweight\n",
        "\n",
        "Real-time predictions become faster\n",
        "\n",
        "(g) Solves the “Curse of Dimensionality”\n",
        "\n",
        "When dimensions increase, distance-based algorithms (like KNN, K-means) become ineffective.\n",
        "Dimensionality reduction helps reduce this effect and improves performance.\n",
        "\n",
        "5. Common Techniques Used for Dimensionality Reduction\n",
        "(1) PCA (Principal Component Analysis)\n",
        "\n",
        "Converts features into new uncorrelated components\n",
        "\n",
        "Retains maximum variance\n",
        "\n",
        "Commonly used in image compression, noise reduction\n",
        "\n",
        "(2) LDA (Linear Discriminant Analysis)\n",
        "\n",
        "Reduces dimensions while maximizing class separation\n",
        "\n",
        "Used in classification problems\n",
        "\n",
        "(3) t-SNE\n",
        "\n",
        "Visualizes high-dimensional data in 2D/3D\n",
        "\n",
        "Popular in clustering and image datasets\n",
        "\n",
        "(4) Autoencoders\n",
        "\n",
        "Neural networks used for non-linear dimensionality reduction\n",
        "\n",
        "Useful in complex datasets\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K8v67pQC4VWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Name and briefly describe three common dimensionality reduction**\n",
        "**techniques.**\n",
        "\n",
        "Answer:-Dimensionality reduction techniques help convert high-dimensional data into a lower-dimensional form while preserving important information. Three widely used techniques are:\n",
        "\n",
        "1. Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is one of the most commonly used feature extraction techniques.\n",
        "It transforms the original features into a new set of uncorrelated variables called principal components.\n",
        "These components are arranged in such a way that:\n",
        "\n",
        "The first component captures the maximum variance in the data.\n",
        "\n",
        "Each next component captures the remaining variance.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Reduces dimensionality while preserving maximum information.\n",
        "\n",
        "Removes correlation between features.\n",
        "\n",
        "Helps in visualization, noise reduction, and image compression.\n",
        "\n",
        "2. Linear Discriminant Analysis (LDA)\n",
        "\n",
        "LDA is a supervised dimensionality reduction method used mainly for classification problems.\n",
        "While PCA focuses on variance, LDA focuses on maximizing class separation.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Finds new axes (linear combinations of features) that best separate different classes.\n",
        "\n",
        "Works only when class labels are available.\n",
        "\n",
        "Useful in face recognition, text classification, and pattern recognition.\n",
        "\n",
        "3. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "t-SNE is a non-linear dimensionality reduction technique mainly used for visualization of high-dimensional data.\n",
        "It converts similarities between data points into probabilities and tries to preserve local structure.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Mostly used to visualize data in 2D or 3D.\n",
        "\n",
        "Excellent for visualizing clusters and patterns.\n",
        "\n",
        "Commonly used in image, text, and biological data (e.g., gene expression).\n",
        "\n",
        "Conclusion\n",
        "\n",
        "These three techniques—PCA, LDA, and t-SNE—are widely used in machine learning for reducing dimensions, improving model performance, and enabling better visualization. Each serves different purposes: PCA for variance preservation, LDA for class separation, and t-SNE for visualization of complex data."
      ],
      "metadata": {
        "id": "PSOJq-Jz5F7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is clustering in unsupervised learning? Mention three popular**\n",
        "**clustering algorithms.**\n",
        "\n",
        "Answer:-1. Introduction\n",
        "\n",
        "Clustering is one of the most important techniques in unsupervised learning, where the goal is to group similar data points together without using predefined labels. It is widely used to discover patterns, hidden structures, or natural groupings in raw data.\n",
        "\n",
        "2. What is Clustering?\n",
        "\n",
        "Clustering is the process of dividing a dataset into multiple groups (called clusters) such that:\n",
        "\n",
        "Data points in the same cluster are very similar to each other\n",
        "\n",
        "Data points in different clusters are dissimilar\n",
        "\n",
        "Similarity is usually measured using distance metrics like:\n",
        "\n",
        "Euclidean distance\n",
        "\n",
        "Manhattan distance\n",
        "\n",
        "Cosine similarity\n",
        "\n",
        "Since clustering is unsupervised, there are no target labels. The algorithm learns patterns from the data itself.\n",
        "\n",
        "3. Why is Clustering Important?\n",
        "\n",
        "Clustering helps in:\n",
        "\n",
        "Pattern recognition\n",
        "\n",
        "Customer segmentation\n",
        "\n",
        "Anomaly detection\n",
        "\n",
        "Image grouping\n",
        "\n",
        "Document/topic grouping\n",
        "\n",
        "Market research\n",
        "\n",
        "It allows organizations to understand data structures intuitively.\n",
        "\n",
        "4. Three Popular Clustering Algorithms\n",
        "1. K-Means Clustering\n",
        "\n",
        "K-Means is one of the most widely used clustering algorithms.\n",
        "It divides the dataset into K distinct clusters based on the mean (centroid) of the data points.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Choose value of K\n",
        "\n",
        "Initialize K centroids\n",
        "\n",
        "Assign each point to nearest centroid\n",
        "\n",
        "Update centroids by averaging assigned points\n",
        "\n",
        "Repeat until convergence\n",
        "\n",
        "Uses: Customer segmentation, grouping images, market basket analysis.\n",
        "\n",
        "2. Hierarchical Clustering\n",
        "\n",
        "Hierarchical clustering builds a tree-like structure (dendrogram) to show how data points cluster together.\n",
        "\n",
        "There are two types:\n",
        "\n",
        "Agglomerative (bottom-up): Start with individual points → merge clusters\n",
        "\n",
        "Divisive (top-down): Start with one cluster → split into smaller ones\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Does not require selecting number of clusters beforehand\n",
        "\n",
        "Produces a visual clustering tree\n",
        "\n",
        "Uses: Bioinformatics, social network analysis, document clustering.\n",
        "\n",
        "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "\n",
        "DBSCAN forms clusters based on the density of data points.\n",
        "\n",
        "Key Ideas:\n",
        "\n",
        "Points in dense regions form clusters\n",
        "\n",
        "Points in sparse regions are labeled as noise or outliers\n",
        "\n",
        "Does not require number of clusters beforehand\n",
        "\n",
        "Works well for irregularly shaped clusters\n",
        "\n",
        "Uses: Fraud detection, spatial data analysis, anomaly detection.\n",
        "\n",
        "5. Conclusion\n",
        "\n",
        "Clustering is a powerful unsupervised learning method used to identify natural groupings in data. Popular algorithms like K-Means, Hierarchical Clustering, and DBSCAN help analyze complex datasets by organizing data into meaningful clusters. Each algorithm has different strengths—K-Means is fast, Hierarchical gives structure, and DBSCAN handles noise and irregular shapes.\n",
        "\n"
      ],
      "metadata": {
        "id": "IotAyeoe5vjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Explain the concept of anomaly detection and its significance.**\n",
        "\n",
        "Answer:-1. Introduction\n",
        "\n",
        "Anomaly detection is an important technique in machine learning used to identify rare, unusual, or abnormal patterns in data that do not follow expected behavior. These unusual events are called anomalies, outliers, or deviations.\n",
        "\n",
        "Anomaly detection is mainly used when abnormal data points may indicate critical incidents such as fraud, system failures, cyber-attacks, or faulty operations.\n",
        "\n",
        "2. What is Anomaly Detection?\n",
        "\n",
        "Anomaly detection is the process of identifying data points that significantly differ from the normal patterns in a dataset.\n",
        "It involves learning the normal behavior of the system and then flagging anything that deviates from it.\n",
        "\n",
        "Anomalies may be:\n",
        "\n",
        "Point Anomalies – A single data point is unusual.\n",
        "Example: A ₹2,00,000 transaction in an account that usually spends ₹5,000.\n",
        "\n",
        "Contextual Anomalies – Abnormal only in a specific context.\n",
        "Example: High temperature in winter is abnormal but normal in summer.\n",
        "\n",
        "Collective Anomalies – A group of data points is abnormal.\n",
        "Example: Sudden spikes in network traffic forming a cyber-attack.\n",
        "\n",
        "3. Methods Used in Anomaly Detection\n",
        "\n",
        "Anomaly detection can be performed using various approaches:\n",
        "\n",
        "(a) Statistical Methods\n",
        "\n",
        "Model normal data distribution and detect points that lie far from expected range.\n",
        "Example: Z-score, Gaussian models.\n",
        "\n",
        "(b) Machine Learning Methods\n",
        "\n",
        "Learn patterns using unsupervised or semi-supervised algorithms.\n",
        "Examples:\n",
        "\n",
        "Isolation Forest\n",
        "\n",
        "One-Class SVM\n",
        "\n",
        "Autoencoders\n",
        "\n",
        "(c) Clustering Methods\n",
        "\n",
        "Assume normal data forms clusters; points outside clusters are anomalies.\n",
        "Examples: K-Means, DBSCAN.\n",
        "\n",
        "4. Why Anomaly Detection is Significant?\n",
        "(1) Fraud Detection\n",
        "\n",
        "Banks and e-wallets use anomaly detection to identify unusual transactions, preventing financial fraud.\n",
        "\n",
        "(2) Cybersecurity\n",
        "\n",
        "Detects abnormal login attempts, unusual network traffic, malware activity, and potential cyber-attacks early.\n",
        "\n",
        "(3) Fault Detection in Machines\n",
        "\n",
        "Monitors machines, sensors, and IoT devices to identify mechanical faults before they lead to breakdown.\n",
        "\n",
        "(4) Healthcare Monitoring\n",
        "\n",
        "Detects abnormal health signals such as irregular heartbeat, sudden spikes in blood pressure, or unusual test results.\n",
        "\n",
        "(5) Quality Control in Manufacturing\n",
        "\n",
        "Identifies defective products or unusual production patterns to maintain quality standards.\n",
        "\n",
        "(6) Predictive Maintenance\n",
        "\n",
        "Helps organizations detect equipment anomalies early and schedule maintenance before major failures occur.\n",
        "\n",
        "(7) Enhanced Decision Making\n",
        "\n",
        "By filtering out unusual or incorrect data, anomaly detection improves data accuracy for analytics and forecasting.\n",
        "\n",
        "5. Importance in Machine Learning and Real-Life Systems\n",
        "\n",
        "Early Warning System: Detects problems before damage occurs.\n",
        "\n",
        "Reduces Losses: Prevents financial loss, downtime, and safety hazards.\n",
        "\n",
        "Improves Security: Anomalies often indicate threats, making systems safer.\n",
        "\n",
        "Supports Automation: Enables automatic detection of unusual situations in large-scale data.\n",
        "\n",
        "Handles Big Data: Useful in environments like banking, telecom, and healthcare where manual monitoring is impossible.\n",
        "\n",
        "6. Conclusion\n",
        "\n",
        "Anomaly detection is a vital machine learning technique that identifies unusual patterns indicating potential risks or important events. Its significance lies in improving security, reliability, and efficiency across various domains like finance, manufacturing, cybersecurity, and healthcare. In modern digital systems, anomaly detection plays a crucial role in safeguarding data, preventing failures, and supporting intelligent automated decision-making."
      ],
      "metadata": {
        "id": "WpS0M58L6YcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: List and briefly describe three types of anomaly detection techniques**.\n",
        "\n",
        "Answer:-Anomaly detection techniques are used to identify data points that deviate significantly from normal patterns. Three commonly used techniques are:\n",
        "\n",
        "1. Statistical Anomaly Detection\n",
        "\n",
        "This method assumes that normal data follows a known statistical distribution (such as Gaussian/Normal distribution). Any data point that falls far outside the expected range is treated as an anomaly.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Uses measures like mean, standard deviation, Z-score, probability density.\n",
        "\n",
        "Identifies points with very low probability of occurrence.\n",
        "\n",
        "Best for simple and well-behaved datasets.\n",
        "\n",
        "Example:\n",
        "\n",
        "Detecting a sudden spike in temperature that lies beyond 3 standard deviations from the mean.\n",
        "\n",
        "2. Machine Learning–Based Anomaly Detection\n",
        "\n",
        "These techniques learn patterns of normal data using machine learning models and identify deviations automatically. They capture complex, non-linear relationships in data.\n",
        "\n",
        "Common Algorithms:\n",
        "\n",
        "Isolation Forest – isolates anomalies rather than profiling normal data\n",
        "\n",
        "One-Class SVM – learns a boundary around normal data\n",
        "\n",
        "Autoencoders (Neural Networks) – reconstruct data; high reconstruction error indicates anomaly\n",
        "\n",
        "Best For:\n",
        "\n",
        "High-dimensional, complex datasets like network logs, financial transactions, or sensor data.\n",
        "\n",
        "3. Clustering-Based Anomaly Detection\n",
        "\n",
        "This method groups normal data into clusters. Data points that do not belong to any cluster or lie far from cluster centers are labeled as anomalies.\n",
        "\n",
        "Common Algorithms:\n",
        "\n",
        "K-Means Clustering – anomalies lie far from centroids\n",
        "\n",
        "DBSCAN – labels low-density points as noise/outliers\n",
        "\n",
        "Hierarchical Clustering – identifies points that do not fit into cluster structure\n",
        "\n",
        "Best For:\n",
        "\n",
        "Datasets where normal data forms dense clusters and anomalies exist in sparse regions.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "These three techniques—Statistical, Machine Learning–Based, and Clustering-Based—form the foundation of anomaly detection. Each method has unique strengths: statistical methods for simple distributions, ML techniques for complex patterns, and clustering methods for density-based anomalies. Together, they help detect fraud, cyber-attacks, faults, and unusual behaviors across various applications.\n",
        "\n"
      ],
      "metadata": {
        "id": "a-R4Vi0T62wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: What is time series analysis? Mention two key components of time series data.**\n",
        "\n",
        "Answer:-1. Introduction\n",
        "\n",
        "Time series analysis is a statistical and machine learning technique used to analyze data that is collected over time at regular intervals. Unlike normal datasets, time series data has a natural time order, and the goal is to study trends, patterns, and future values.\n",
        "\n",
        "Common examples of time series include:\n",
        "\n",
        "Daily stock prices\n",
        "\n",
        "Monthly sales data\n",
        "\n",
        "Hourly temperature readings\n",
        "\n",
        "Website traffic per day\n",
        "\n",
        "2. What is Time Series Analysis?\n",
        "\n",
        "Time series analysis refers to the process of understanding, modeling, and forecasting data that changes over time. It involves:\n",
        "\n",
        "Identifying patterns such as trend, seasonality, or cycles\n",
        "\n",
        "Studying how past values influence current and future values\n",
        "\n",
        "Making forecasts for future time periods\n",
        "\n",
        "Time series analysis helps in planning, forecasting, decision-making, and detecting unusual behavior in temporal data.\n",
        "\n",
        "3. Importance of Time Series Analysis\n",
        "\n",
        "Helps businesses predict sales, revenue, and demand\n",
        "\n",
        "Used in weather forecasting and climate studies\n",
        "\n",
        "Detects anomalies and sudden changes\n",
        "\n",
        "Supports financial market analysis and investment decisions\n",
        "\n",
        "4. Two Key Components of Time Series Data\n",
        "1. Trend\n",
        "\n",
        "Trend represents the long-term direction in which the data is moving over time.\n",
        "It can be:\n",
        "\n",
        "Upward trend (e.g., increasing sales over years)\n",
        "\n",
        "Downward trend (e.g., decreasing rainfall over decades)\n",
        "\n",
        "No trend (data remains stable)\n",
        "\n",
        "Trend shows the overall movement of the data, ignoring short-term fluctuations.\n",
        "\n",
        "2. Seasonality\n",
        "\n",
        "Seasonality refers to repeating patterns or fluctuations that occur at regular, predictable intervals.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Higher ice cream sales in summer\n",
        "\n",
        "More electricity usage during winter\n",
        "\n",
        "Weekly website traffic patterns\n",
        "\n",
        "Seasonality helps identify periodic behaviors influenced by seasons, months, days, or events.\n",
        "\n",
        "5. Other Components (for completeness)\n",
        "\n",
        "Though the question asks for only two, time series also includes:\n",
        "\n",
        "Cyclic patterns (long-term irregular cycles)\n",
        "\n",
        "Irregular/Random noise (unpredictable variations)\n",
        "\n",
        "6. Conclusion\n",
        "\n",
        "Time series analysis is a powerful technique used to understand how data evolves over time and make accurate forecasts. The two key components—trend and seasonality—help identify long-term direction and periodic patterns, making time series essential for business planning, forecasting, and decision-making"
      ],
      "metadata": {
        "id": "VanpocGn7aZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Describe the difference between seasonality and cyclic behavior in time series**\n",
        "\n",
        "Answer:-Time series data often contains repeating patterns that help in understanding and forecasting future values. Two important components of these patterns are seasonality and cyclic behavior. Although both involve fluctuations over time, they differ in duration, predictability, and underlying causes.\n",
        "\n",
        "1. Seasonality\n",
        "\n",
        "Seasonality refers to regular, repeating patterns that occur at fixed and predictable intervals within a time series.\n",
        "\n",
        "Key Characteristics of Seasonality\n",
        "\n",
        "Fixed period: Occurs in consistent intervals (e.g., hourly, daily, weekly, monthly, yearly).\n",
        "\n",
        "Predictable pattern: The pattern repeats in the same manner every period.\n",
        "\n",
        "Driven by known factors: Typically influenced by climate, holidays, festivals, or human behavior.\n",
        "\n",
        "Short-term nature: Usually repeats within a year or less.\n",
        "\n",
        "Examples\n",
        "\n",
        "Increase in ice cream sales every summer.\n",
        "\n",
        "High electricity usage during winter.\n",
        "\n",
        "Weekend spikes in online shopping.\n",
        "\n",
        "2. Cyclic Behavior\n",
        "\n",
        "Cyclic behavior refers to long-term fluctuations in a time series that do not occur at fixed intervals. Cycles depend on economic, social, or natural processes that span multiple years.\n",
        "\n",
        "Key Characteristics of Cyclic Patterns\n",
        "\n",
        "No fixed period: The duration of cycles varies and is unpredictable.\n",
        "\n",
        "Irregular repetition: Cycles occur but may differ in length, amplitude, and timing.\n",
        "\n",
        "Driven by economic or social forces: Typically linked to business cycles, market conditions, political changes, or natural events.\n",
        "\n",
        "Long-term nature: Cycles often last several years.\n",
        "\n",
        "Examples\n",
        "\n",
        "Business economic cycles (boom → recession → recovery).\n",
        "\n",
        "Real estate market cycles.\n",
        "\n",
        "Long-term climate cycles like El Niño.\n",
        "\n",
        "\n",
        "\n",
        "| **Feature**        | **Seasonality**                    | **Cyclic Behavior**                 |\n",
        "| ------------------ | ---------------------------------- | ----------------------------------- |\n",
        "| **Period**         | Fixed and known                    | Variable and unknown                |\n",
        "| **Predictability** | Highly predictable                 | Not strictly predictable            |\n",
        "| **Duration**       | Short-term (within 1 year)         | Long-term (several years)           |\n",
        "| **Cause**          | Climate, festivals, human behavior | Economic, social, or natural forces |\n",
        "| **Pattern**        | Repeats in same form               | Repeats but not in the same manner  |\n",
        "\n",
        "\n",
        "4. Conclusion\n",
        "\n",
        "Seasonality and cyclic behavior both represent recurring patterns in time series analysis, but they differ in their frequency, predictability, and underlying causes. Seasonality is short-term and predictable with fixed intervals, whereas cyclic behavior is long-term and irregular. Understanding the distinction helps analysts build better forecasting models and make accurate data-driven decisions."
      ],
      "metadata": {
        "id": "mqRJvABT75IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write Python code to perform K-means clustering on a sample dataset**.**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "GZ6s1I_c8One"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create sample dataset\n",
        "X, y = make_blobs(n_samples=100, centers=3, random_state=42)\n",
        "\n",
        "# Apply K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Output cluster centers and labels\n",
        "centers = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_\n",
        "\n",
        "centers, labels[:20]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwvTEHJI8t_8",
        "outputId": "b328fab4-61e5-42f0-9f9b-e2555856fe3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-2.66780392,  8.93576069],\n",
              "        [-6.95170962, -6.67621669],\n",
              "        [ 4.49951001,  1.93892013]]),\n",
              " array([1, 2, 0, 2, 1, 2, 0, 2, 2, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1],\n",
              "       dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: What is inheritance in OOP? Provide a simple example in Python.**\n",
        "\n",
        "Answer:-1. Definition\n",
        "\n",
        "Inheritance is one of the core concepts of Object-Oriented Programming (OOP).\n",
        "It allows a class (called the child class or subclass) to acquire the properties and methods of another class (called the parent class or superclass).\n",
        "\n",
        "In simple words, inheritance allows code reusability by enabling new classes to use the features of existing classes without rewriting them.\n",
        "\n",
        "2. Why Inheritance is Useful?\n",
        "\n",
        "Promotes code reusability\n",
        "\n",
        "Makes programs easier to maintain\n",
        "\n",
        "Helps follow DRY (Don’t Repeat Yourself) principle\n",
        "\n",
        "Allows creation of hierarchical class structures\n",
        "\n",
        "Supports method overriding (child class modifies parent method)\n",
        "\n",
        "3. Types of Inheritance in Python\n",
        "\n",
        "Single inheritance\n",
        "\n",
        "Multiple inheritance\n",
        "\n",
        "Multilevel inheritance\n",
        "\n",
        "Hierarchical inheritance\n",
        "\n",
        "Hybrid inheritance\n",
        "\n",
        "(You can mention briefly in exams.)\n",
        "\n",
        "\n",
        "\n",
        "5. Explanation of Example\n",
        "\n",
        "The class Animal is the parent class with a method sound().\n",
        "\n",
        "The class Dog is the child class that inherits from Animal.\n",
        "\n",
        "The Dog class can access both:\n",
        "\n",
        "sound() method from the parent\n",
        "\n",
        "its own bark() method\n",
        "\n",
        "The child class object d demonstrates inheritance by using parent class features.\n",
        "\n",
        "6. Conclusion\n",
        "\n",
        "Inheritance is a key feature of OOP that enables child classes to reuse and extend the functionality of parent classes. It improves code structure, reduces redundancy, and supports polymorphism. The Python example shows how a child class can inherit and use methods defined in a parent class."
      ],
      "metadata": {
        "id": "gKAN2E5b9h6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parent class\n",
        "class Animal:\n",
        "    def sound(self):\n",
        "        return \"Animals make sounds\"\n",
        "\n",
        "# Child class inheriting from Animal\n",
        "class Dog(Animal):\n",
        "    def bark(self):\n",
        "        return \"Dog barks: Woof Woof!\"\n",
        "\n",
        "# Creating an object of child class\n",
        "d = Dog()\n",
        "\n",
        "print(d.sound())  # Inherited from parent class\n",
        "print(d.bark())   # Child class own method\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mk30Oiq9zJi",
        "outputId": "3f9baa95-a9dd-4049-d16f-d1dd29bba6f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Animals make sounds\n",
            "Dog barks: Woof Woof!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: How can time series analysis be used for anomaly detection?**\n",
        "\n",
        "Answer:-Time series analysis helps detect anomalies by examining how data changes over time and identifying points that do not follow the expected pattern based on historical behavior.\n",
        "\n",
        "How it works:\n",
        " 1. Detect deviations from trends\n",
        "\n",
        "If a value suddenly jumps far above or drops below the expected trend, it is flagged as an anomaly.\n",
        "Example: Daily sales suddenly drop to zero.\n",
        "\n",
        "2. Detect unexpected changes in seasonality\n",
        "\n",
        "If a time series usually shows repeated patterns (daily/weekly/annual), any break in the pattern is an anomaly.\n",
        "Example: Electricity usage is always high at night, but suddenly drops.\n",
        "\n",
        "3. Detect unusual residuals using forecasting models\n",
        "\n",
        "Methods like ARIMA, Prophet, LSTM can predict future values.\n",
        "If the actual value is far from the predicted value → anomaly.\n",
        "\n",
        "Predicted: 100\n",
        "\n",
        "Actual: 250\n",
        "\n",
        "Difference is too large → anomaly detected.\n",
        "\n",
        "Common Time Series Anomaly Detection Approaches:\n",
        "\n",
        "Statistical (Z-score, Rolling mean/SD):\n",
        "Flag points that fall outside normal range.\n",
        "\n",
        "Forecast-based models (ARIMA, Prophet):\n",
        "Compare actual vs. predicted values.\n",
        "\n",
        "Machine Learning models (LSTM Autoencoders):\n",
        "Models learn normal patterns and detect unusual behavior.\n",
        "\n",
        "Why it is useful?\n",
        "\n",
        "Detect fraud (bank transactions)\n",
        "\n",
        "Monitor server performance (CPU spikes)\n",
        "\n",
        "Identify sensor failures (IoT devices)\n",
        "\n",
        "Track demand changes (retail, energy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0I1hnNeT91QW"
      }
    }
  ]
}